NumPy 2.0 Migration
    To be done when min supported version becomes 2.0:
        pyxu.operator.interop.jax.py
            xp.byte_bounds() -> xp.lib.array_utils.byte_bounds()

doc/
    api/ has/is-being updated to ND API interactively.

pyxu.operator.linop.fft.filter
    FFTConvolve/FFTCorrelate overpad for boundary conditions != "constant". 
        Currently there is two padding actions done in the input array:
            1. op._pad() pads the N-sized input array to N_pad with the desired boundary conditions (reflect, periodic, etc.), 
            2. fft's in the _stencil_chain used apply/adjoint further zero-pad to shape (N_pad+K-1). 

pyxu.abc.solver
    Allow not writing data to disk at end.
        writeback_rate = None -> don't write (ever)
                         -1   -> only at convergence
                          N   -> every N-th iteration
    Replace .npz file storage with ZARR format.
        Add test_disk_chunk_matches_memory() [but this will never be tested since we disabled DASK solvers?]
            We should instantiate only one solver to verify this.

pyxu.opt.nlcg
    Refactor to ND API. (2023.12.29: not started yet.)
        Update test_nlcg.py
    Need to ensure math.backtracking_linesearch() works beforehand.
        Fix first. (See associated issue below.)

pyxu.math.backtracking_linesearch()
    Add test suite under math/linesearch/test_backtrack.py
        Testing this function requires similar code to conftest.FuncT().
            Wrap it into a Func() and piggy-back on FuncT().
                Extra args go into __init__()
            Disable all irrelevant tests as required.
    Known to be broken for DASK inputs ... and maybe for NUMPY/CUPY too.
        Fix it guided by test suite.
    Known to be algorithmically inefficient for DASK inputs.
        Call pxw.warn_dask_perf()

pyxu.opt.adam
    Refactor to ND API. (2023.12.29: not started yet.)
        Update test_adam.py

pyxu.opt.pds
    Remove CP, FB, PP? They are only passing args to CV or PD3O (other solvers rely on CV or PD3O but have different step sizes).

pyxu.operator.interop.jax
    Code + test suite has been ported to ND API.
    [2023.12.29] CPU tests mostly pass, except a few items in
        TestJax[PSD,SelfAdjoint]Convolution
            Hypothesis: when porting to Jax, convolution implemented incorrectly.
            We copied what was done in CircularConvolution, but an error may have been introduced in the process.
        TestJaxScaleDown
            svdvals()-errors?
    Implementations of asarray() and _quad_spec() were simplified, but not sure if it broke anything.
        Correctness of asarray() is simple to verify via the test suite.
        Correctness of _quad_spec() is not assessed since a suitable test structure differs significantly from the other Operator classes.
            Assess correctness on small instantiations.

    Implement to_jax(Operator) to make Pyxu operators available to the JAX ecosystem.
        Idea: to_jax() returns a NamedTuple of following fields (subset of arithmetic methods):
            def apply(arr: jax.Array) -> jax.Array                  # (..., M1,...,MD) -> (..., N1,...,NK)
            def grad(arr: jax.Array) -> jax.Array                   # (..., M1,...,MD) -> (..., M1,...,MD)
            def adjoint(arr: jax.Array) -> jax.Array                # (..., N1,...,NK) -> (..., M1,...,MD)
            def prox(arr: jax.Array, tau: pxt.Real) -> jax.Array    # (..., M1,...,MD) -> (..., M1,...,MD)
            def pinv(arr: jax.Array, damp: pxt.Real) -> jax.Array   # (..., N1,...,NK) -> (..., M1,...,MD)
        Need to create these methods by encapsulating Pyxu functions.

pyxu.operator.interop.torch
    [2024.03.02] Updated to ND API
    The PyTorch interface is slightly different from the JAX interface; it would be ideal to homogenize them.
    Add a test suite similar to test_jax.py

pyxu.operator.blocks
    Long-term plan for blocks:
        Pyxu is 2 things:
            * A collection of efficient *scalable* operators to work on NDArrays.
            * A collection of solvers and other niceities to solve optimization problems.

        We already achieve both these goals with the current API.

        One downside however is that the operator arithmetic is strongly tied to the (array-in -> array-out) assumption.
        This means:
            * we cannot have operators which take multiple inputs and return multiple outputs; instead one needs
              currently to flatten/stack input/outputs to feed them to operators.
            * solvers return ONE output: it is not possible to optimize different parameters simultaneously without
              flat/stack-ing them all. (Consequence of former point.)

        One solution would be to replace arithmetic methods in the Operator API to instead accept list[NDArray] inputs and output list[NDArray].
        Solvers would then accept list[NDArray] initial points & save list[NDArray] to disk.
        The issue with this solution is that most (if not all) core operators have no meaning on list[NDArray] inputs. (Ex: FFT of a list; what does that mean exactly? etc.)
        Therefore list[NDArray] in/outputs are a consequence of operator arithmetic only, and solvers need to be able to handle these in/output pairs.

        [Proposition] introduce a CompositeOperator class with the following interface:

            class CompositeOp(Operator):
                def __init__(ops)
                    ops: CompositeOp -> no-op
                    ops: list[Operator] -> see op()

                def op() -> Namespace()
                    Identify all operators involved in creating `ops`.
                        Form the full operator chain, then prune nodes which are not required to compute the desired outputs.
                    Infer global operator type (LinOp, etc.) based on the global graph.
                    Redefine arithmetic methods to eval all terms in parallel via
                        [NUMPY/CUPY] concurrent.futures
                        [DASK] Its own task graph
                    Arithmetic methods now all take/return a special Array() class.
                        Array = thin layer around dict[str, NDArray] with simple rules for +/-/*[for scaling]

            This interface requires ZERO changes to the current arithmetic API.
            The only difference is that solvers can take either Operator or CompositeOp as input and handle outputs accordingly.
            Concretely all Operators should be cast to CompositeOperator instances; and solver m_init/m_step handle composite operators only.
            Solver must be updated to write multiple arrays to disk.

            Composite op completely replaces blocks.py and will be part of abc/.

pyxu.operator.linop.kron
    Refactor to ND API. (2024.01.01: not started yet.)
        Update test_kron.py

pyxu.operator.linop.fft.czt
    There are known closed forms for gram()/cogram(), hence pinv(): implement them.

Known Issues: Cause Unknown (Investigate)
    TestTan::test_transparent_call
    TestTanh::test_math_diff_lipschitz
    TestArcCosh::test_math_lipschitz
    TestArcCosh::test_math_diff_lipschitz
    TestSign::test_math_lipschitz
    TestHyperSlab::test_math_prox
    TestL1Ball::test_backend_fenchel_prox
    TestL1Ball::test_backend_prox
    TestL1Ball::test_chunk_fenchel_prox
    TestL1Ball::test_chunk_prox
    TestL1Ball::test_math_prox
    TestL1Ball::test_math1_moreau_envelope
    TestL1Ball::test_math2_moreau_envelope
    TestL1Ball::test_prec_fenchel_prox
    TestL1Ball::test_prec_prox
    TestL1Ball::test_precCM_fenchel_prox
    TestL1Ball::test_precCM_prox
    TestL1Ball::test_transparent_fenchel_prox
    TestL1Ball::test_transparent_prox
    TestL1Ball::test_value1D_fenchel_prox
    TestL1Ball::test_value1D_prox
    TestL1Ball::test_valueND_fenchel_prox
    TestL1Ball::test_valueND_prox
    TestRangeSet::test_chunk_prox
    TestRangeSet::test_transparent_apply
    TestRangeSet::test_transparent_call
    TestRangeSet::test_value1D_apply
    TestRangeSet::test_value1D_call
    TestRangeSet::test_valueND_apply
    TestRangeSet::test_valueND_call
    TestCZT::test_chunk_pinv [hangs for DASK inputs]
Known Issues: Cause Known (& Fixable?)
    [OK] Due to DASK-suboptimal tensordot()
        TestExplicitLinXX::test_chunk_adjoint
        TestExplicitLinXX::test_chunk_apply
        TestExplicitLinXX::test_chunk_call
        TestExplicitLinXX::test_chunk_fenchel_prox
        TestExplicitLinXX::test_chunk_grad
        TestExplicitLinXX::test_chunk_pinv
    [OK] DASK-case: Fixture[_op_asarray, width=DOUBLE] does not denote the true ground-truth due to DASK-based FFT()
         error accumulation.  Not really a problem because errors are in 1e-7 range, but did not look into how to fix
         them yet.
        TestFFT::test_value_asarray
        TestCZT::test_value_asarray
